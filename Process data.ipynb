{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3580e5b-6237-4785-b1df-941698bc9574",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Reading data from raw container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdab64d5-f2a5-4cb8-b496-f02af452227b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").option(\"header\", \"true\").load(\"/mnt/raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c102123-e7c5-4875-9f67-f42bc6c1bcf6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Casting Columns to Date Datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "100e96ab-9cfb-4707-b381-9c7c27384d21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"Open_Date\", col(\"Open_Date\").cast(\"Date\"))\n",
    "df = df.withColumn(\"Last_Consulted_Date\", col(\"Last_Consulted_Date\").cast(\"Date\"))\n",
    "df = df.withColumn(\"DOB\", col(\"DOB\").cast(\"Date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15f6d663-45f0-4358-b462-4b118143a816",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Adding new column age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c01fa81f-aa7e-4661-b169-45d3e6c3d1e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, current_date, floor\n",
    "\n",
    "df = df.withColumn(\"age\", floor(datediff(current_date(), col(\"DOB\")) / 365))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45823fa4-0db9-41ac-bbc2-879aa2b9ec81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Adding new column days_since_last_consulted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da729e9e-dbbd-4bee-9d35-155ab42abda9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, current_date\n",
    "\n",
    "df = df.withColumn(\"days_since_last_consulted\", datediff(current_date(), col(\"Last_Consulted_Date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8aa95d9-ccf5-439e-8599-8b1cf81194be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "distinct_countries = df.select(\"country\").distinct().rdd.map(lambda row: row[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06af7893-88d2-4ca4-8cf7-5312239bfe16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Consider the latest record for a customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f64ce869-a033-47d2-b3ff-8df8d25a9cbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "# Define window spec\n",
    "windowSpec = Window.partitionBy(\"Customer_Id\").orderBy(col(\"Last_Consulted_Date\").desc())\n",
    "\n",
    "# Add a row number for each row within each partition of Customer_Id\n",
    "df_with_row_number = df.withColumn(\"row_number\", row_number().over(windowSpec))\n",
    "\n",
    "# Filter rows to get the most recent Last_Consulted_Date for each Customer_Id\n",
    "df_recent = df_with_row_number.filter(col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "# Filter rows to get older records for each Customer_Id\n",
    "df_older = df_with_row_number.filter(col(\"row_number\") > 1).drop(\"row_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfc3ddfa-fe9a-448c-8dfb-5379bf8796fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### writing data to individual country directories in ADLS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98ddf789-9f46-42ef-a80b-2364ddd5747c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for country in distinct_countries:\n",
    "    country_df = df_recent.filter(df_recent['country'] == country)\n",
    "    country_path = f\"/mnt/processed/{country}\"\n",
    "    country_df.write.mode(\"overwrite\").parquet(country_path)\n",
    "\n",
    "df_older.write.mode(\"overwrite\").parquet(\"/mnt/processed/older\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Process data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
